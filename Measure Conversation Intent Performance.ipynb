{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Conversation Intent Performance\n",
    "\n",
    "The following notebook details the steps to test the performance of intent classificaiton within an IBM Watson Conversation workspace.\n",
    "\n",
    "What you'll need:\n",
    "\n",
    "1. A conversation workspace file (named workspace.json)\n",
    "2. A set of example utterences with which to test (named test_set.csv)\n",
    "3. The credentials of your conversation service and the workspace_id (named conv_creds.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the conversation workspace\n",
    "\n",
    "In order to carry out these tests you will need an instance of the IBM Watson Conversation service and a workspace that has been trained with two or more intents. For details of the service and how to go about this please see the getting started section of the documentation.\n",
    "\n",
    "For the purpose of this notebook it is assumed that you already have the service instance and a trained workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required dependancies\n",
    "\n",
    "We'll be using a number of dependancies including the watson_developer_cloud package which you can find and install from here. Additionally we'll be using the scikit-learn metrics to help display a report and the matplotlib for output a fancy confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json, time\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from watson_developer_cloud import ConversationV1\n",
    "from IPython.display import clear_output\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Using the pandas package import the data we'll be using for our test. The expected format here is a two column csv with \"example utterences\", \"label\", if your file is in a different format you'll need to change the code below, the aim is to get three arrays, test_X that contains the list if utterences, test_Y which contains the labels for each item in test_X and labels which contains the unique list of labels for all utterences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"test_set.csv\", quotechar='\"', skipinitialspace=True)\n",
    "print(dataset.shape)\n",
    "\n",
    "# separate the data from the target attributes\n",
    "test_X = dataset.values[:,0].tolist()\n",
    "test_Y = dataset.values[:,1].tolist()\n",
    "\n",
    "# Get a unique list of the labels in this test set\n",
    "labels = np.unique(test_Y)\n",
    "pprint(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the service credentials\n",
    "\n",
    "You will need a file in the same directory as this notebook that contains the information in the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"credentials\": {\n",
    "        \"url\": \"https://gateway.watsonplatform.net/conversation/api\",\n",
    "        \"password\": \"PASSWORD\",\n",
    "        \"username\": \"USERNAME\",\n",
    "        \"workspace_id\": \"WORKSPACE_ID\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "To get the url, username and password for your instance go to the Bluemix dashboard, find the service instance you want and select it then see the Service Credentials tab. Your workspace_id can be obtained by launching the tooling then selecting the menu icon in the top right of the workspace tile and selecting details.\n",
    "\n",
    "If you change the format of your credentials file you'll need to change the code below to get the details from your file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"conv_creds.json\") as f:\n",
    "    creds = json.load(f)['credentials']\n",
    "    username,password = creds['username'], creds['password']\n",
    "    endpoint = creds['url']\n",
    "    workspace_id = creds['workspace_id']\n",
    "\n",
    "print(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test set\n",
    "\n",
    "For each item in the test set that is in the array test_X we'll hit the Conversations API passing in the utterences as the input. The intent name of the first intent returned for each utterences in then added to an array (preds) for use later.\n",
    "\n",
    "A running total of the tests completed is output and when the tests are complete \"Completed test run\" will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate over each example in the test set\n",
    "conversation = ConversationV1(\n",
    "  username=username,\n",
    "  password=password,\n",
    "  version='2016-09-20'\n",
    ")\n",
    "preds = []\n",
    "\n",
    "for idx, item in enumerate(test_X):\n",
    "    response = conversation.message(\n",
    "      workspace_id=workspace_id,\n",
    "      message_input={'text': item},\n",
    "      context=None\n",
    "    )\n",
    "    preds.append(response[\"intents\"][0][\"intent\"])\n",
    "    clear_output()\n",
    "    print(\"Completed {!s} of {!s}\\r\".format(idx+1, len(test_X)))\n",
    "\n",
    "print(\"Completed test run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the test metrics\n",
    "\n",
    "We're using the classification report from the scikit-learn.metrics module to help us display the performance of the classifier. We could roll our own metric calcualtions but really why reinvent the wheel?\n",
    "\n",
    "The report show us the precision, recall, f1-score and support (number of items tested) fro each class and as a total. For more information on exactly what these measures mean see the scikit-learn website.\n",
    "\n",
    "Note the call to \"np.unique(np.append(test_Y, preds))\" this is needed as sometimes the class labels predicted are not in the list of labels we are testing. In a perfect world you would have tests for all possible intents but that isn't always the case so this expression ensures that the report runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_Y, preds,\n",
    "                                    target_names=np.unique(np.append(test_Y, preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a confusion matrix\n",
    "\n",
    "Confusion matricies can be hard to read, essentially you're looking to identify the labels that are incorrectly classified and what those examples are actually classified as. If I have four labels (1,2,3,4) and my confusion matrix displays that examples of label 2 are often classified as label 3 then I would know that the two labels overlap and I need to look deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncats = len(np.unique(np.append(test_Y, preds)))\n",
    "import matplotlib \n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 15.0)\n",
    "plt.matshow( metrics.confusion_matrix(test_Y, preds), cmap='cubehelix', );\n",
    "plt.colorbar();\n",
    "plt.xticks(range(ncats), np.unique(np.append(test_Y, preds)), rotation=90);\n",
    "plt.yticks(range(ncats), np.unique(np.append(test_Y, preds)));\n",
    "plt.ylabel(\"True Label\");\n",
    "plt.xlabel(\"Predicted Label\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
